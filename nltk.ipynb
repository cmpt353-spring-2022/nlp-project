{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8753e620",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ce5569",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "#nltk.config_megam('megam.opt')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d529f87e",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62afe677",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filtered = pd.read_csv('data/filtered.csv.gz')\n",
    "#display(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e457226a",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cb74ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "def filter_stopwords(wordlist):\n",
    "    filtered = []\n",
    "    \n",
    "    for word in wordlist:\n",
    "        if word not in stopwords:\n",
    "            filtered.append(word)\n",
    "    \n",
    "    return filtered\n",
    "\n",
    "def lemmatize_words(wordlist):\n",
    "    lemmatized = []\n",
    "    \n",
    "    for word in wordlist:\n",
    "        lemmatized.append(nltk.stem.wordnet.WordNetLemmatizer().lemmatize(word))\n",
    "    \n",
    "    return lemmatized\n",
    "\n",
    "def prepare_titles(data):\n",
    "    data['title'] = data['title'].str.lower().str.strip()\n",
    "    data = data[data['title'] != '']\n",
    "    data.dropna(subset='title', inplace=True)\n",
    "            \n",
    "    data['title'] = data['title'].apply(lambda x: nltk.word_tokenize(x))\n",
    "    #data['title'] = data['title'].apply(lambda x: filter_stopwords(x)) # lowers accuracy\n",
    "    data['title'] = data['title'].apply(lambda x: lemmatize_words(x))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bc8c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_feature(wordlist):\n",
    "    feature = {}\n",
    "    \n",
    "    for x in all_features:\n",
    "        feature[x] = x in wordlist\n",
    "        \n",
    "    return feature\n",
    "\n",
    "def create_featuresets(data, train, num_features=100):\n",
    "    #if train:\n",
    "    #    document = [(row['title'], row['clickbait']) for index, row in data.iterrows()]\n",
    "    #else:\n",
    "    #    document = [(row['title']) for index, row in data.iterrows()]\n",
    "    \n",
    "    all_words = []\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        for word in row['title']:\n",
    "            all_words.append(word)\n",
    "\n",
    "    if train:\n",
    "        global all_features\n",
    "        all_features = list(nltk.FreqDist(all_words))[:num_features]\n",
    "        \n",
    "        # vectorizing using numpy shaves off about 40% of the processing time for this section\n",
    "        \n",
    "        featuresets = np.array(data[['title', 'clickbait']])\n",
    "        find_vector = np.vectorize(find_feature)\n",
    "        featuresets[:,0] = find_vector(featuresets[:,0])\n",
    "    else:\n",
    "        featuresets = np.array(data[['title']])\n",
    "        find_vector = np.vectorize(find_feature)\n",
    "        featuresets[:,0] = find_vector(featuresets[:,0])\n",
    "    \n",
    "    #if train:\n",
    "    #    featuresets = [(find_feature(wordlist), category) for (wordlist, category) in document]\n",
    "    #else:\n",
    "    #    featuresets = [(find_feature(wordlist)) for (wordlist) in document]\n",
    "        \n",
    "    return featuresets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9df0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data, train, num_feaures=100):\n",
    "    #time0 = time.time()\n",
    "    data = prepare_titles(data)\n",
    "    #print('prepare_titles time: {:.2f}s'.format(time.time() - time0))\n",
    "    \n",
    "    # used for error analysis below\n",
    "    #global temp_datacopy\n",
    "    #temp_datacopy = data.copy()\n",
    "    \n",
    "    #time0 = time.time()\n",
    "    data = create_featuresets(data, train, num_feaures)\n",
    "    #print('create_featuresets time: {:.2f}s'.format(time.time() - time0))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94044653",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "prepared = prepare_data(filtered, True, 500)\n",
    "train_ratio = 0.7\n",
    "train, test = prepared[:int(len(prepared) * train_ratio)], prepared[int(len(prepared) * train_ratio):]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bd8291",
   "metadata": {},
   "source": [
    "# Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f84a7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "time0 = time.time()\n",
    "nbclassifier = nltk.NaiveBayesClassifier.train(train)\n",
    "#print('time: {:.2f}s'.format(time.time() - time0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc803a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trains too slowly\n",
    "#time0 = time.time()\n",
    "#meclassifier_iis = nltk.MaxentClassifier.train(train, algorithm='iis', max_iter=5)\n",
    "#print('\\ntraining duration: {:.2f}s'.format(time.time() - time0))\n",
    "\n",
    "# trains too slowly\n",
    "#time0 = time.time()\n",
    "#meclassifier_gis = nltk.MaxentClassifier.train(train, algorithm='gis', max_iter=5)\n",
    "#print('\\ntraining duration: {:.2f}s'.format(time.time() - time0))\n",
    "\n",
    "# much less accurate than NaiveBayesClassifier\n",
    "#time0 = time.time()\n",
    "#meclassifier_megam = nltk.MaxentClassifier.train(train, algorithm='megam')\n",
    "#print('\\ntraining duration: {:.2f}s'.format(time.time() - time0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f8a9a4",
   "metadata": {},
   "source": [
    "# Test Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc0a824",
   "metadata": {},
   "outputs": [],
   "source": [
    "#time0 = time.time()\n",
    "accuracy = nltk.classify.accuracy(nbclassifier, test)\n",
    "#print('accuracy: {:.4f}, time: {:.2f}s'.format(accuracy, time.time() - time0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf4cf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# used for error analysis\n",
    "\n",
    "#errors = []\n",
    "#index = 0\n",
    "\n",
    "#for (words, clickbait) in train:\n",
    "#    prediction = nbclassifier.classify(words)\n",
    "    \n",
    "#    if prediction != clickbait:\n",
    "#        errors.append((clickbait, temp_datacopy['title'].iloc[index], filtered['title'].iloc[index]))\n",
    "\n",
    "#    index += 1\n",
    "    \n",
    "#for e in errors:\n",
    "#    if e[0] == 1:\n",
    "#        print('clickbait: yes')\n",
    "#    else:\n",
    "#        print('clickbait: no')\n",
    "\n",
    "#    print(e[2])\n",
    "#    print(e[3] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307efde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trains too slowly\n",
    "#time0 = time.time()\n",
    "#print(nltk.classify.accuracy(meclassifier_iis, test))\n",
    "#print('\\ntesting duration: {:.2f}s'.format(time.time() - time0))\n",
    "\n",
    "# trains too slowly\n",
    "#time0 = time.time()\n",
    "#print(nltk.classify.accuracy(meclassifier_gis, test))\n",
    "#print('\\ntesting duration: {:.2f}s'.format(time.time() - time0))\n",
    "\n",
    "# much less accurate than NaiveBayesClassifier\n",
    "#time0 = time.time()\n",
    "#print(nltk.classify.accuracy(meclassifier_megam, test))\n",
    "#print('\\ntesting duration: {:.2f}s'.format(time.time() - time0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd2e3d6",
   "metadata": {},
   "source": [
    "# Classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a0c2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['data/nottheonion_lg.csv.gz', \n",
    "         'data/politics_lg.csv.gz', \n",
    "         'data/upliftingnews_lg.csv.gz', \n",
    "         'data/worldnews_lg.csv.gz']#, \n",
    "         #'data/news_lg.csv.gz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c627264",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "\n",
    "for file in files:\n",
    "    dfs.append(pd.read_csv(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e68a05",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filtered_dfs = []\n",
    "\n",
    "for df in dfs:\n",
    "    df = df[df['score'] > 50]\n",
    "    df = df[['created_utc', 'title']]\n",
    "    \n",
    "    if type(df['created_utc'].iloc[0]) == str:\n",
    "        df['created_utc'] = pd.to_datetime(df['created_utc'], format='%Y-%m-%d %H:%M:%S')\n",
    "    else:\n",
    "        df['created_utc'] = pd.to_datetime(df['created_utc'], unit='s')\n",
    "        \n",
    "    df = df[df['created_utc'].dt.year > 2013]\n",
    "    \n",
    "    filtered_dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a27bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = []\n",
    "\n",
    "for df in filtered_dfs:\n",
    "    df = prepare_data(df, False)\n",
    "    featuresets.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d04385",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_classify(featureset):\n",
    "    return nltk.NaiveBayesClassifier.classify(nbclassifier, featureset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba02141",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_classify = np.vectorize(custom_classify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904b9cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions = filtered_dfs.copy()\n",
    "\n",
    "for i in range(len(featuresets)):\n",
    "    predictions = np.array(featuresets[i])\n",
    "    predictions = vector_classify(predictions)\n",
    "    df_predictions[i]['nb_predictions'] = predictions\n",
    "    df_predictions[i].to_csv('data/nltk_predictions_' + str(i) + '.csv.gz', index=False, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740ef31e",
   "metadata": {},
   "source": [
    "# Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702cc7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(data, thresh):\n",
    "    temp = data.copy()\n",
    "    temp['created_utc'] = temp['created_utc'].dt.year\n",
    "    \n",
    "    result_types = ['nb']\n",
    "    result = pd.DataFrame()\n",
    "    \n",
    "    for rt in result_types:\n",
    "        count = temp.pivot_table(index='created_utc', columns=rt + '_predictions', aggfunc='size')\n",
    "        count[rt + '_ratio'] = count[1] / (count[1] + count[0])\n",
    "        count = count[(count[1] + count[0]) > thresh]\n",
    "        result = result.join(count[[rt + '_ratio']], how='right')\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30050ed6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "for df in df_predictions:\n",
    "    result = visualize_results(df, 150)\n",
    "    plt.plot(result * 100)\n",
    "\n",
    "plt.title('Percentage of clickbait titles in selected news subreddits (2014-2021)')\n",
    "plt.legend(['r/NotTheOnion', 'r/politics', 'r/UpliftingNews', 'r/worldnews'])\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Percentage (%)')\n",
    "plt.savefig('nltk_analysis.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c704c184",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
